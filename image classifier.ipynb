{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e80f9b8-57cf-4729-a414-3bd1c453959a",
   "metadata": {},
   "source": [
    "# About the Dataset\n",
    "\n",
    "## Context\n",
    "\n",
    "The dataset is derived from a growing e-commerce industry and provides a rich source of information for analysis and research. It includes:\n",
    "\n",
    "- **High-Resolution Product Images**: Professionally captured images showcasing each product.\n",
    "- **Label Attributes**: Multiple attributes describing the product, manually entered during cataloging.\n",
    "- **Descriptive Text**: Comments and descriptions detailing the product characteristics.\n",
    "\n",
    "This dataset offers a comprehensive view of products, including visual, categorical, and descriptive information, making it ideal for various analyses such as image classification, attribute prediction, and product recommendation.\n",
    "\n",
    "## Content\n",
    "\n",
    "The dataset is structured as follows:\n",
    "\n",
    "1. **Product Identification**:\n",
    "   - Each product is uniquely identified by an ID (e.g., 42431).\n",
    "\n",
    "2. **Styles File**:\n",
    "   - **File Name**: `styles.csv`\n",
    "   - **Content**: Contains a mapping of product IDs to various attributes and categories. This file helps in associating product IDs with their respective attributes and categories.\n",
    "\n",
    "## Objective\n",
    "\n",
    "The goal is to build an image classifier that can accurately classify product images into their respective master categories. This involves leveraging high-resolution product images and the `styles.csv` file to train and validate the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f54b5964-7960-42ac-9c62-3b0724656bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "from PIL import Image\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "303855e9-884d-4996-9aa9-9e5d62030192",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('styles.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ea1ace38-1c99-4e17-b086-5c8779e784af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>masterCategory</th>\n",
       "      <th>subCategory</th>\n",
       "      <th>articleType</th>\n",
       "      <th>baseColour</th>\n",
       "      <th>season</th>\n",
       "      <th>year</th>\n",
       "      <th>usage</th>\n",
       "      <th>productDisplayName</th>\n",
       "      <th>productDisplayName1</th>\n",
       "      <th>productDisplayName2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15970</td>\n",
       "      <td>Men</td>\n",
       "      <td>Apparel</td>\n",
       "      <td>Topwear</td>\n",
       "      <td>Shirts</td>\n",
       "      <td>Navy Blue</td>\n",
       "      <td>Fall</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>Casual</td>\n",
       "      <td>Turtle Check Men Navy Blue Shirt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>39386</td>\n",
       "      <td>Men</td>\n",
       "      <td>Apparel</td>\n",
       "      <td>Bottomwear</td>\n",
       "      <td>Jeans</td>\n",
       "      <td>Blue</td>\n",
       "      <td>Summer</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>Casual</td>\n",
       "      <td>Peter England Men Party Blue Jeans</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>59263</td>\n",
       "      <td>Women</td>\n",
       "      <td>Accessories</td>\n",
       "      <td>Watches</td>\n",
       "      <td>Watches</td>\n",
       "      <td>Silver</td>\n",
       "      <td>Winter</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>Casual</td>\n",
       "      <td>Titan Women Silver Watch</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21379</td>\n",
       "      <td>Men</td>\n",
       "      <td>Apparel</td>\n",
       "      <td>Bottomwear</td>\n",
       "      <td>Track Pants</td>\n",
       "      <td>Black</td>\n",
       "      <td>Fall</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>Casual</td>\n",
       "      <td>Manchester United Men Solid Black Track Pants</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>53759</td>\n",
       "      <td>Men</td>\n",
       "      <td>Apparel</td>\n",
       "      <td>Topwear</td>\n",
       "      <td>Tshirts</td>\n",
       "      <td>Grey</td>\n",
       "      <td>Summer</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>Casual</td>\n",
       "      <td>Puma Men Grey T-shirt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44441</th>\n",
       "      <td>17036</td>\n",
       "      <td>Men</td>\n",
       "      <td>Footwear</td>\n",
       "      <td>Shoes</td>\n",
       "      <td>Casual Shoes</td>\n",
       "      <td>White</td>\n",
       "      <td>Summer</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>Casual</td>\n",
       "      <td>Gas Men Caddy Casual Shoe</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44442</th>\n",
       "      <td>6461</td>\n",
       "      <td>Men</td>\n",
       "      <td>Footwear</td>\n",
       "      <td>Flip Flops</td>\n",
       "      <td>Flip Flops</td>\n",
       "      <td>Red</td>\n",
       "      <td>Summer</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>Casual</td>\n",
       "      <td>Lotto Men's Soccer Track Flip Flop</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44443</th>\n",
       "      <td>18842</td>\n",
       "      <td>Men</td>\n",
       "      <td>Apparel</td>\n",
       "      <td>Topwear</td>\n",
       "      <td>Tshirts</td>\n",
       "      <td>Blue</td>\n",
       "      <td>Fall</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>Casual</td>\n",
       "      <td>Puma Men Graphic Stellar Blue Tshirt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44444</th>\n",
       "      <td>46694</td>\n",
       "      <td>Women</td>\n",
       "      <td>Personal Care</td>\n",
       "      <td>Fragrance</td>\n",
       "      <td>Perfume and Body Mist</td>\n",
       "      <td>Blue</td>\n",
       "      <td>Spring</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>Casual</td>\n",
       "      <td>Rasasi Women Blue Lady Perfume</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44445</th>\n",
       "      <td>51623</td>\n",
       "      <td>Women</td>\n",
       "      <td>Accessories</td>\n",
       "      <td>Watches</td>\n",
       "      <td>Watches</td>\n",
       "      <td>Pink</td>\n",
       "      <td>Winter</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>Casual</td>\n",
       "      <td>Fossil Women Pink Dial Chronograph Watch ES3050</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>44446 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id gender masterCategory subCategory            articleType  \\\n",
       "0      15970    Men        Apparel     Topwear                 Shirts   \n",
       "1      39386    Men        Apparel  Bottomwear                  Jeans   \n",
       "2      59263  Women    Accessories     Watches                Watches   \n",
       "3      21379    Men        Apparel  Bottomwear            Track Pants   \n",
       "4      53759    Men        Apparel     Topwear                Tshirts   \n",
       "...      ...    ...            ...         ...                    ...   \n",
       "44441  17036    Men       Footwear       Shoes           Casual Shoes   \n",
       "44442   6461    Men       Footwear  Flip Flops             Flip Flops   \n",
       "44443  18842    Men        Apparel     Topwear                Tshirts   \n",
       "44444  46694  Women  Personal Care   Fragrance  Perfume and Body Mist   \n",
       "44445  51623  Women    Accessories     Watches                Watches   \n",
       "\n",
       "      baseColour  season    year   usage  \\\n",
       "0      Navy Blue    Fall  2011.0  Casual   \n",
       "1           Blue  Summer  2012.0  Casual   \n",
       "2         Silver  Winter  2016.0  Casual   \n",
       "3          Black    Fall  2011.0  Casual   \n",
       "4           Grey  Summer  2012.0  Casual   \n",
       "...          ...     ...     ...     ...   \n",
       "44441      White  Summer  2013.0  Casual   \n",
       "44442        Red  Summer  2011.0  Casual   \n",
       "44443       Blue    Fall  2011.0  Casual   \n",
       "44444       Blue  Spring  2017.0  Casual   \n",
       "44445       Pink  Winter  2016.0  Casual   \n",
       "\n",
       "                                    productDisplayName productDisplayName1  \\\n",
       "0                     Turtle Check Men Navy Blue Shirt                 NaN   \n",
       "1                   Peter England Men Party Blue Jeans                 NaN   \n",
       "2                             Titan Women Silver Watch                 NaN   \n",
       "3        Manchester United Men Solid Black Track Pants                 NaN   \n",
       "4                                Puma Men Grey T-shirt                 NaN   \n",
       "...                                                ...                 ...   \n",
       "44441                        Gas Men Caddy Casual Shoe                 NaN   \n",
       "44442               Lotto Men's Soccer Track Flip Flop                 NaN   \n",
       "44443             Puma Men Graphic Stellar Blue Tshirt                 NaN   \n",
       "44444                   Rasasi Women Blue Lady Perfume                 NaN   \n",
       "44445  Fossil Women Pink Dial Chronograph Watch ES3050                 NaN   \n",
       "\n",
       "      productDisplayName2  \n",
       "0                     NaN  \n",
       "1                     NaN  \n",
       "2                     NaN  \n",
       "3                     NaN  \n",
       "4                     NaN  \n",
       "...                   ...  \n",
       "44441                 NaN  \n",
       "44442                 NaN  \n",
       "44443                 NaN  \n",
       "44444                 NaN  \n",
       "44445                 NaN  \n",
       "\n",
       "[44446 rows x 12 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f56239-c256-454b-835e-f862147cd4d1",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA) Guide\n",
    "\n",
    "## Objective\n",
    "\n",
    "The goal is to clean and prepare the dataset for analysis by aggregating product display columns and mapping image paths to product IDs.\n",
    "\n",
    "## Steps\n",
    "\n",
    "### 1. Data Aggregation\n",
    "\n",
    "The dataset has three columns for product display due to improper CSV encoding. We need to combine these into a single column.\n",
    "\n",
    "### 2. Image Extraction and Mapping\n",
    "\n",
    "We extract images from a zip file and then link each image to its respective product ID. This is done by using the image filenames to determine the product ID and then creating a new column in the dataset that includes the file paths to these images.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5beafb82-d0f3-48d4-b119-0eeeb45c63f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['productDisplayName1'] = df['productDisplayName1'].fillna('')\n",
    "df['productDisplayName2'] = df['productDisplayName1'].fillna('')\n",
    "df['final product name'] = df['productDisplayName'] + \", \"+df['productDisplayName1'] + \", \"+df['productDisplayName2'] \n",
    "df['final product name'] = df['final product name'].str.rstrip(', , ')\n",
    "df = df.drop(['productDisplayName', 'productDisplayName1', 'productDisplayName2'], axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "dc15ae5e-0815-4702-a736-ba17833dd7f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>masterCategory</th>\n",
       "      <th>subCategory</th>\n",
       "      <th>articleType</th>\n",
       "      <th>baseColour</th>\n",
       "      <th>season</th>\n",
       "      <th>year</th>\n",
       "      <th>usage</th>\n",
       "      <th>final product name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15970</td>\n",
       "      <td>Men</td>\n",
       "      <td>Apparel</td>\n",
       "      <td>Topwear</td>\n",
       "      <td>Shirts</td>\n",
       "      <td>Navy Blue</td>\n",
       "      <td>Fall</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>Casual</td>\n",
       "      <td>Turtle Check Men Navy Blue Shirt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>39386</td>\n",
       "      <td>Men</td>\n",
       "      <td>Apparel</td>\n",
       "      <td>Bottomwear</td>\n",
       "      <td>Jeans</td>\n",
       "      <td>Blue</td>\n",
       "      <td>Summer</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>Casual</td>\n",
       "      <td>Peter England Men Party Blue Jeans</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>59263</td>\n",
       "      <td>Women</td>\n",
       "      <td>Accessories</td>\n",
       "      <td>Watches</td>\n",
       "      <td>Watches</td>\n",
       "      <td>Silver</td>\n",
       "      <td>Winter</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>Casual</td>\n",
       "      <td>Titan Women Silver Watch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21379</td>\n",
       "      <td>Men</td>\n",
       "      <td>Apparel</td>\n",
       "      <td>Bottomwear</td>\n",
       "      <td>Track Pants</td>\n",
       "      <td>Black</td>\n",
       "      <td>Fall</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>Casual</td>\n",
       "      <td>Manchester United Men Solid Black Track Pants</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>53759</td>\n",
       "      <td>Men</td>\n",
       "      <td>Apparel</td>\n",
       "      <td>Topwear</td>\n",
       "      <td>Tshirts</td>\n",
       "      <td>Grey</td>\n",
       "      <td>Summer</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>Casual</td>\n",
       "      <td>Puma Men Grey T-shirt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44441</th>\n",
       "      <td>17036</td>\n",
       "      <td>Men</td>\n",
       "      <td>Footwear</td>\n",
       "      <td>Shoes</td>\n",
       "      <td>Casual Shoes</td>\n",
       "      <td>White</td>\n",
       "      <td>Summer</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>Casual</td>\n",
       "      <td>Gas Men Caddy Casual Shoe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44442</th>\n",
       "      <td>6461</td>\n",
       "      <td>Men</td>\n",
       "      <td>Footwear</td>\n",
       "      <td>Flip Flops</td>\n",
       "      <td>Flip Flops</td>\n",
       "      <td>Red</td>\n",
       "      <td>Summer</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>Casual</td>\n",
       "      <td>Lotto Men's Soccer Track Flip Flop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44443</th>\n",
       "      <td>18842</td>\n",
       "      <td>Men</td>\n",
       "      <td>Apparel</td>\n",
       "      <td>Topwear</td>\n",
       "      <td>Tshirts</td>\n",
       "      <td>Blue</td>\n",
       "      <td>Fall</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>Casual</td>\n",
       "      <td>Puma Men Graphic Stellar Blue Tshirt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44444</th>\n",
       "      <td>46694</td>\n",
       "      <td>Women</td>\n",
       "      <td>Personal Care</td>\n",
       "      <td>Fragrance</td>\n",
       "      <td>Perfume and Body Mist</td>\n",
       "      <td>Blue</td>\n",
       "      <td>Spring</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>Casual</td>\n",
       "      <td>Rasasi Women Blue Lady Perfume</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44445</th>\n",
       "      <td>51623</td>\n",
       "      <td>Women</td>\n",
       "      <td>Accessories</td>\n",
       "      <td>Watches</td>\n",
       "      <td>Watches</td>\n",
       "      <td>Pink</td>\n",
       "      <td>Winter</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>Casual</td>\n",
       "      <td>Fossil Women Pink Dial Chronograph Watch ES3050</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>44446 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id gender masterCategory subCategory            articleType  \\\n",
       "0      15970    Men        Apparel     Topwear                 Shirts   \n",
       "1      39386    Men        Apparel  Bottomwear                  Jeans   \n",
       "2      59263  Women    Accessories     Watches                Watches   \n",
       "3      21379    Men        Apparel  Bottomwear            Track Pants   \n",
       "4      53759    Men        Apparel     Topwear                Tshirts   \n",
       "...      ...    ...            ...         ...                    ...   \n",
       "44441  17036    Men       Footwear       Shoes           Casual Shoes   \n",
       "44442   6461    Men       Footwear  Flip Flops             Flip Flops   \n",
       "44443  18842    Men        Apparel     Topwear                Tshirts   \n",
       "44444  46694  Women  Personal Care   Fragrance  Perfume and Body Mist   \n",
       "44445  51623  Women    Accessories     Watches                Watches   \n",
       "\n",
       "      baseColour  season    year   usage  \\\n",
       "0      Navy Blue    Fall  2011.0  Casual   \n",
       "1           Blue  Summer  2012.0  Casual   \n",
       "2         Silver  Winter  2016.0  Casual   \n",
       "3          Black    Fall  2011.0  Casual   \n",
       "4           Grey  Summer  2012.0  Casual   \n",
       "...          ...     ...     ...     ...   \n",
       "44441      White  Summer  2013.0  Casual   \n",
       "44442        Red  Summer  2011.0  Casual   \n",
       "44443       Blue    Fall  2011.0  Casual   \n",
       "44444       Blue  Spring  2017.0  Casual   \n",
       "44445       Pink  Winter  2016.0  Casual   \n",
       "\n",
       "                                    final product name  \n",
       "0                     Turtle Check Men Navy Blue Shirt  \n",
       "1                   Peter England Men Party Blue Jeans  \n",
       "2                             Titan Women Silver Watch  \n",
       "3        Manchester United Men Solid Black Track Pants  \n",
       "4                                Puma Men Grey T-shirt  \n",
       "...                                                ...  \n",
       "44441                        Gas Men Caddy Casual Shoe  \n",
       "44442               Lotto Men's Soccer Track Flip Flop  \n",
       "44443             Puma Men Graphic Stellar Blue Tshirt  \n",
       "44444                   Rasasi Women Blue Lady Perfume  \n",
       "44445  Fossil Women Pink Dial Chronograph Watch ES3050  \n",
       "\n",
       "[44446 rows x 10 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c2b3baa2-0250-4b23-9b91-4134e0749b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "23343674-deb4-4d43-8775-a03c6d04ec38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "zip_path = 'images.zip'\n",
    "with ZipFile(zip_path) as myzip:\n",
    "    files_in_zip = myzip.namelist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5257cb50-9cb9-4274-ad7f-bef0eaf10109",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = pd.DataFrame(files_in_zip, columns = ['Image Name'])\n",
    "image['id'] = pd.Series(image['Image Name']).str.extract('(\\d+)')\n",
    "image['id'] = image['id'].astype(int)\n",
    "final_df = pd.merge(df, image, how = 'inner', on = 'id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a9eed1f3-0904-4bc6-afe9-8201bd1597e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>masterCategory</th>\n",
       "      <th>subCategory</th>\n",
       "      <th>articleType</th>\n",
       "      <th>baseColour</th>\n",
       "      <th>season</th>\n",
       "      <th>year</th>\n",
       "      <th>usage</th>\n",
       "      <th>final product name</th>\n",
       "      <th>Image Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15970</td>\n",
       "      <td>Men</td>\n",
       "      <td>Apparel</td>\n",
       "      <td>Topwear</td>\n",
       "      <td>Shirts</td>\n",
       "      <td>Navy Blue</td>\n",
       "      <td>Fall</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>Casual</td>\n",
       "      <td>Turtle Check Men Navy Blue Shirt</td>\n",
       "      <td>images/15970.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>39386</td>\n",
       "      <td>Men</td>\n",
       "      <td>Apparel</td>\n",
       "      <td>Bottomwear</td>\n",
       "      <td>Jeans</td>\n",
       "      <td>Blue</td>\n",
       "      <td>Summer</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>Casual</td>\n",
       "      <td>Peter England Men Party Blue Jeans</td>\n",
       "      <td>images/39386.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>59263</td>\n",
       "      <td>Women</td>\n",
       "      <td>Accessories</td>\n",
       "      <td>Watches</td>\n",
       "      <td>Watches</td>\n",
       "      <td>Silver</td>\n",
       "      <td>Winter</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>Casual</td>\n",
       "      <td>Titan Women Silver Watch</td>\n",
       "      <td>images/59263.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21379</td>\n",
       "      <td>Men</td>\n",
       "      <td>Apparel</td>\n",
       "      <td>Bottomwear</td>\n",
       "      <td>Track Pants</td>\n",
       "      <td>Black</td>\n",
       "      <td>Fall</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>Casual</td>\n",
       "      <td>Manchester United Men Solid Black Track Pants</td>\n",
       "      <td>images/21379.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>53759</td>\n",
       "      <td>Men</td>\n",
       "      <td>Apparel</td>\n",
       "      <td>Topwear</td>\n",
       "      <td>Tshirts</td>\n",
       "      <td>Grey</td>\n",
       "      <td>Summer</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>Casual</td>\n",
       "      <td>Puma Men Grey T-shirt</td>\n",
       "      <td>images/53759.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44436</th>\n",
       "      <td>17036</td>\n",
       "      <td>Men</td>\n",
       "      <td>Footwear</td>\n",
       "      <td>Shoes</td>\n",
       "      <td>Casual Shoes</td>\n",
       "      <td>White</td>\n",
       "      <td>Summer</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>Casual</td>\n",
       "      <td>Gas Men Caddy Casual Shoe</td>\n",
       "      <td>images/17036.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44437</th>\n",
       "      <td>6461</td>\n",
       "      <td>Men</td>\n",
       "      <td>Footwear</td>\n",
       "      <td>Flip Flops</td>\n",
       "      <td>Flip Flops</td>\n",
       "      <td>Red</td>\n",
       "      <td>Summer</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>Casual</td>\n",
       "      <td>Lotto Men's Soccer Track Flip Flop</td>\n",
       "      <td>images/6461.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44438</th>\n",
       "      <td>18842</td>\n",
       "      <td>Men</td>\n",
       "      <td>Apparel</td>\n",
       "      <td>Topwear</td>\n",
       "      <td>Tshirts</td>\n",
       "      <td>Blue</td>\n",
       "      <td>Fall</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>Casual</td>\n",
       "      <td>Puma Men Graphic Stellar Blue Tshirt</td>\n",
       "      <td>images/18842.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44439</th>\n",
       "      <td>46694</td>\n",
       "      <td>Women</td>\n",
       "      <td>Personal Care</td>\n",
       "      <td>Fragrance</td>\n",
       "      <td>Perfume and Body Mist</td>\n",
       "      <td>Blue</td>\n",
       "      <td>Spring</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>Casual</td>\n",
       "      <td>Rasasi Women Blue Lady Perfume</td>\n",
       "      <td>images/46694.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44440</th>\n",
       "      <td>51623</td>\n",
       "      <td>Women</td>\n",
       "      <td>Accessories</td>\n",
       "      <td>Watches</td>\n",
       "      <td>Watches</td>\n",
       "      <td>Pink</td>\n",
       "      <td>Winter</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>Casual</td>\n",
       "      <td>Fossil Women Pink Dial Chronograph Watch ES3050</td>\n",
       "      <td>images/51623.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>44441 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id gender masterCategory subCategory            articleType  \\\n",
       "0      15970    Men        Apparel     Topwear                 Shirts   \n",
       "1      39386    Men        Apparel  Bottomwear                  Jeans   \n",
       "2      59263  Women    Accessories     Watches                Watches   \n",
       "3      21379    Men        Apparel  Bottomwear            Track Pants   \n",
       "4      53759    Men        Apparel     Topwear                Tshirts   \n",
       "...      ...    ...            ...         ...                    ...   \n",
       "44436  17036    Men       Footwear       Shoes           Casual Shoes   \n",
       "44437   6461    Men       Footwear  Flip Flops             Flip Flops   \n",
       "44438  18842    Men        Apparel     Topwear                Tshirts   \n",
       "44439  46694  Women  Personal Care   Fragrance  Perfume and Body Mist   \n",
       "44440  51623  Women    Accessories     Watches                Watches   \n",
       "\n",
       "      baseColour  season    year   usage  \\\n",
       "0      Navy Blue    Fall  2011.0  Casual   \n",
       "1           Blue  Summer  2012.0  Casual   \n",
       "2         Silver  Winter  2016.0  Casual   \n",
       "3          Black    Fall  2011.0  Casual   \n",
       "4           Grey  Summer  2012.0  Casual   \n",
       "...          ...     ...     ...     ...   \n",
       "44436      White  Summer  2013.0  Casual   \n",
       "44437        Red  Summer  2011.0  Casual   \n",
       "44438       Blue    Fall  2011.0  Casual   \n",
       "44439       Blue  Spring  2017.0  Casual   \n",
       "44440       Pink  Winter  2016.0  Casual   \n",
       "\n",
       "                                    final product name        Image Name  \n",
       "0                     Turtle Check Men Navy Blue Shirt  images/15970.jpg  \n",
       "1                   Peter England Men Party Blue Jeans  images/39386.jpg  \n",
       "2                             Titan Women Silver Watch  images/59263.jpg  \n",
       "3        Manchester United Men Solid Black Track Pants  images/21379.jpg  \n",
       "4                                Puma Men Grey T-shirt  images/53759.jpg  \n",
       "...                                                ...               ...  \n",
       "44436                        Gas Men Caddy Casual Shoe  images/17036.jpg  \n",
       "44437               Lotto Men's Soccer Track Flip Flop   images/6461.jpg  \n",
       "44438             Puma Men Graphic Stellar Blue Tshirt  images/18842.jpg  \n",
       "44439                   Rasasi Women Blue Lady Perfume  images/46694.jpg  \n",
       "44440  Fossil Women Pink Dial Chronograph Watch ES3050  images/51623.jpg  \n",
       "\n",
       "[44441 rows x 11 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "28502acc-d48c-4ea1-abd3-7470a6c01972",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df['Image Name'] = \"imagedata/\"+final_df['Image Name']\n",
    "old_substring = 'imagedata/images/'\n",
    "new_substring = 'C:\\\\Users\\\\RishiGupta\\\\OneDrive - ASPECTRATIO PRIVATE LIMITED\\\\Documents\\\\data science all\\\\myntradataset\\\\images'\n",
    "\n",
    "# Replace only the specified substrin\n",
    "final_df['Image Name'] = final_df['Image Name'].str.replace(old_substring, new_substring, regex=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89068f4-4d6a-4dff-a9b9-559e2fda5250",
   "metadata": {},
   "source": [
    "## Handling Dataset Imbalance and Sampling\n",
    "\n",
    "### Dataset Imbalance\n",
    "\n",
    "Upon analyzing the distribution of master categories, we discovered that the dataset is imbalanced, with some classes having significantly more samples than others. This imbalance can affect model performance and training efficiency.\n",
    "\n",
    "### Sampling Strategy\n",
    "\n",
    "To manage this imbalance and to accommodate the constraints of running the model on a CPU, we will sample 5,000 images from the dataset. This approach helps to reduce the computational load while ensuring a manageable dataset size.\n",
    "\n",
    "### Ensuring Class Representation\n",
    "\n",
    "To maintain an accurate representation of all classes in the sample:\n",
    "- **Weighted Sampling**: Use the `sample` function in the pandas library with weights to ensure that each class is proportionally represented in the sample. This ensures that even less frequent classes are included in the sampled dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d8ef692d-212f-49a8-a418-23119dcf205e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "masterCategory\n",
       "Apparel           21395\n",
       "Accessories       11289\n",
       "Footwear           9222\n",
       "Personal Care      2404\n",
       "Free Items          105\n",
       "Sporting Goods       25\n",
       "Home                  1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df['masterCategory'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b9157ba2-d8b8-4292-a65e-e38705e86c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = final_df[final_df['masterCategory'] != 'Home']\n",
    "data = {\n",
    "    'masterCategory': ['Apparel', 'Accessories', 'Footwear', 'Personal Care', 'Free Items', 'Sporting Goods'],\n",
    "    'Count': [21395, 11289, 9222, 2404, 105, 25]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "df['Proportion'] = 1/(df['Count'] / df['Count'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e4bd7ed1-ce86-484c-8a76-580b46088ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.merge(final_df, df, how = 'inner', on = 'masterCategory')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "00a22f2d-6608-4ae1-ab82-bef3c9abe179",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df_sample = final_df.sample(n = 5000, weights = 'Proportion', random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "663fe0c6-9aa1-4503-ae9e-7850daa61d67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "masterCategory\n",
       "Apparel           1340\n",
       "Accessories       1259\n",
       "Footwear          1227\n",
       "Personal Care     1044\n",
       "Free Items         105\n",
       "Sporting Goods      25\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df_sample['masterCategory'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66f3eab-76e5-4514-adcf-f8467779a5db",
   "metadata": {},
   "source": [
    "## Dataset Preparation for Neural Network\r\n",
    "\r\n",
    "### 1. Label Encoding\r\n",
    "\r\n",
    "We use a label encoder to convert categorical master categories into numerical values. This step is essential for transforming the textual categories into a format suitable for machine learning models.\r\n",
    "\r\n",
    "### 2. Data Splitting\r\n",
    "\r\n",
    "The dataset is split into three subsets:\r\n",
    "- **Training Set (60%)**: Used to train the neural network.\r\n",
    "- **Validation Set (20%)**: Used to tune hyperparameters and evaluate model performance during training.\r\n",
    "- **Test Set (20%)**: Used to assess the final model's performance on unseen data.\r\n",
    "\r\n",
    "### 3. Data Normalization\r\n",
    "\r\n",
    "Images are normalized to a range of [0,1] to improve convergence during neural network training. This normalization step ensures that the pixel values are scaled uniformly, which helps the model learn more effectively.\r\n",
    "\r\n",
    "### 4. Data Loading and Batching\r\n",
    "\r\n",
    "The dataset is loaded into a TensorFlow `Dataset` object, with images processed and batched for training. We divide the data into batches of 32 images to manage memory usage and accelerate training. Batching allows for more efficient processing and faster convergence of the model.\r\n",
    "\r\n",
    "This preparation process ensures that the dataset is well-structured and optimized for training a neural network, with proper encoding, splitting, normalization, and batching of data.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "5226e8e8-37bd-4a32-a3eb-5bc836af7896",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "final_df_sample['masterCategory'] = le.fit_transform(final_df_sample['masterCategory'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "8957df71-429b-4a70-9d0b-c50f6b1a59fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_val, X_test = train_test_split(final_df_sample, test_size=0.2, stratify= final_df_sample['masterCategory'], random_state=42)\n",
    "X_train, X_val = train_test_split(X_train_val, test_size=0.25, stratify= X_train_val['masterCategory'], random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "cb980521-1663-4929-8fe2-454825cc1e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(final_df_sample):\n",
    "    category = final_df_sample['masterCategory'].tolist()\n",
    "    names = final_df_sample['Image Name'].tolist()\n",
    "    def load_and_preprocess_image(image_path, target_size=(224, 224)):\n",
    "        image = tf.io.read_file(image_path)\n",
    "        image = tf.image.decode_jpeg(image, channels=3)\n",
    "        image = tf.image.resize(image, target_size)\n",
    "        image = image / 255.0 \n",
    "        image.set_shape([target_size[0], target_size[1], 3])\n",
    "        return image\n",
    "\n",
    "    def process_image_and_label(image_path, label):\n",
    "        image = load_and_preprocess_image(image_path)\n",
    "        return image, label\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((names, category))\n",
    "    dataset = dataset.map(lambda x, y: process_image_and_label(x, y), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    batch_size = 32\n",
    "    dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "da37edb3-c9b1-4ca6-b458-031bbde42452",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = load_data(X_train)\n",
    "dataset_val = load_data(X_val)\n",
    "dataset_test = load_data(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14ef996-ca60-4ff0-aaa7-29d377878acf",
   "metadata": {},
   "source": [
    "## Model Definition and Training\r\n",
    "\r\n",
    "### 1. Model Architecture\r\n",
    "\r\n",
    "We employ transfer learning with ResNet-50 as the base model. ResNet-50 is chosen for several reasons:\r\n",
    "\r\n",
    "- **Feature Extraction**: ResNet-50 excels at extracting features from images due to its deep residual network architecture. This architecture includes shortcut connections that help the network learn complex features more effectively by addressing the vanishing gradient problem.\r\n",
    "  \r\n",
    "- **Pre-Trained Weights**: ResNet-50 comes with pre-trained weights from large-scale datasets like ImageNet. These pre-trained weights capture a wide range of visual features, which can be beneficial for our specific task even if our dataset is smaller or different in nature.\r\n",
    "\r\n",
    "- **Residual Connections**: The residual connections in ResNet-50 allow the network to train deeper models without degradation in performance. This is achieved by enabling gradients to flow more effectively through the network, which improves feature learning and overall model accuracy.\r\n",
    "\r\n",
    "- **Efficient Training**: Using ResNet-50 as a base model reduces the need for training a deep neural network from scratch, saving computational resources and time. It also leverages the extensive training done on large datasets to enhance the performance of our model.\r\n",
    "\r\n",
    "### 2. Model Configuration\r\n",
    "\r\n",
    "- **Base Model**: \r\n",
    "  - We use ResNet-50 as a pre-trained feature extractor.\r\n",
    "  - The layers of ResNet-50 are kept frozen (not updated during training) to preserve the learned features. \r\n",
    "\r\n",
    "- **Top Neural Network**:\r\n",
    "  - A custom neural network is added on top of ResNet-50.\r\n",
    "  - This network is specifically trained for our image classification task, allowing the model to adapt the extracted features to our particular dataset.\r\n",
    "\r\n",
    "### 3. Optimizer\r\n",
    "\r\n",
    "We use the Adam optimizer for training:\r\n",
    "- **Adam Optimizer**: \r\n",
    "  - Adam is chosen for its ability to adapt the learning rate dynamically based on the model's performance.\r\n",
    "  - It adjusts the learning rate during training, which helps in faster convergence and improved accuracy.\r\n",
    "\r\n",
    "### 4. Training Configuration\r\n",
    "\r\n",
    "- **Number of Epochs**: \r\n",
    "  - The model is trained for 25 epochs.\r\n",
    "  - This duration allows the model to learn effectively from the dataset while balancing training time and performance.\r\n",
    "\r\n",
    "By leveraging ResNet-50’s advanced feature extraction capabilities and using the Adam optimizer, we create a robust model that efficiently learns to classify images while benefiting from the extensive knowledge embedded in the pre-trained ResNet-50 model.\r\n",
    "mage classification task.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 659,
   "id": "36e8a72c-9ba9-4edf-9573-c334e5cd9d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "base_model = tf.keras.applications.ResNet50(\n",
    "    include_top=False,\n",
    "    weights=\"imagenet\",\n",
    "    input_tensor=None,\n",
    "    input_shape=(224, 224, 3),\n",
    "    pooling='avg',\n",
    ")\n",
    "base_model.trainable = False\n",
    "model.add(base_model)\n",
    "model.add(Dense(units=40, activation='relu'))\n",
    "model.add(Dense(units=25, activation='relu'))\n",
    "model.add(Dense(units=10, activation='relu'))\n",
    "model.add(Dense(units=6, activation='linear'))\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.007)\n",
    "model.compile(loss = SparseCategoricalCrossentropy(from_logits = True), optimizer=opt, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "id": "c0122c9b-c7d1-4963-96cb-5551d3d1f5e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 2s/step - accuracy: 0.3142 - loss: 1.4912 - val_accuracy: 0.5920 - val_loss: 1.0685\n",
      "Epoch 2/25\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m166s\u001b[0m 2s/step - accuracy: 0.5854 - loss: 1.0616 - val_accuracy: 0.6680 - val_loss: 0.9030\n",
      "Epoch 3/25\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m167s\u001b[0m 2s/step - accuracy: 0.6516 - loss: 0.8891 - val_accuracy: 0.6600 - val_loss: 0.8768\n",
      "Epoch 4/25\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m173s\u001b[0m 2s/step - accuracy: 0.6781 - loss: 0.9028 - val_accuracy: 0.8120 - val_loss: 0.6194\n",
      "Epoch 5/25\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m175s\u001b[0m 2s/step - accuracy: 0.7632 - loss: 0.6854 - val_accuracy: 0.8220 - val_loss: 0.5638\n",
      "Epoch 6/25\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m165s\u001b[0m 2s/step - accuracy: 0.7897 - loss: 0.6238 - val_accuracy: 0.8310 - val_loss: 0.5318\n",
      "Epoch 7/25\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m165s\u001b[0m 2s/step - accuracy: 0.7992 - loss: 0.5888 - val_accuracy: 0.8290 - val_loss: 0.5133\n",
      "Epoch 8/25\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m165s\u001b[0m 2s/step - accuracy: 0.8021 - loss: 0.5736 - val_accuracy: 0.8400 - val_loss: 0.4987\n",
      "Epoch 9/25\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m166s\u001b[0m 2s/step - accuracy: 0.8113 - loss: 0.5424 - val_accuracy: 0.8420 - val_loss: 0.4814\n",
      "Epoch 10/25\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m167s\u001b[0m 2s/step - accuracy: 0.8122 - loss: 0.5394 - val_accuracy: 0.8460 - val_loss: 0.4778\n",
      "Epoch 11/25\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m168s\u001b[0m 2s/step - accuracy: 0.8256 - loss: 0.5247 - val_accuracy: 0.8480 - val_loss: 0.4798\n",
      "Epoch 12/25\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m167s\u001b[0m 2s/step - accuracy: 0.8276 - loss: 0.5077 - val_accuracy: 0.7630 - val_loss: 0.6939\n",
      "Epoch 13/25\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m165s\u001b[0m 2s/step - accuracy: 0.8104 - loss: 0.5862 - val_accuracy: 0.7690 - val_loss: 0.6598\n",
      "Epoch 14/25\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m165s\u001b[0m 2s/step - accuracy: 0.8017 - loss: 0.5772 - val_accuracy: 0.7980 - val_loss: 0.5876\n",
      "Epoch 15/25\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m165s\u001b[0m 2s/step - accuracy: 0.8159 - loss: 0.5435 - val_accuracy: 0.8340 - val_loss: 0.5249\n",
      "Epoch 16/25\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m165s\u001b[0m 2s/step - accuracy: 0.8193 - loss: 0.5279 - val_accuracy: 0.8450 - val_loss: 0.4890\n",
      "Epoch 17/25\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m165s\u001b[0m 2s/step - accuracy: 0.8213 - loss: 0.5016 - val_accuracy: 0.8440 - val_loss: 0.5018\n",
      "Epoch 18/25\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m164s\u001b[0m 2s/step - accuracy: 0.8257 - loss: 0.5214 - val_accuracy: 0.8150 - val_loss: 0.5645\n",
      "Epoch 19/25\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m211s\u001b[0m 2s/step - accuracy: 0.8241 - loss: 0.5170 - val_accuracy: 0.8570 - val_loss: 0.4762\n",
      "Epoch 20/25\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m212s\u001b[0m 2s/step - accuracy: 0.8235 - loss: 0.5058 - val_accuracy: 0.8610 - val_loss: 0.4658\n",
      "Epoch 21/25\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m211s\u001b[0m 2s/step - accuracy: 0.8323 - loss: 0.4677 - val_accuracy: 0.8460 - val_loss: 0.4919\n",
      "Epoch 22/25\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m210s\u001b[0m 2s/step - accuracy: 0.8307 - loss: 0.5044 - val_accuracy: 0.8710 - val_loss: 0.4332\n",
      "Epoch 23/25\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m210s\u001b[0m 2s/step - accuracy: 0.8171 - loss: 0.5174 - val_accuracy: 0.8440 - val_loss: 0.4932\n",
      "Epoch 24/25\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m170s\u001b[0m 2s/step - accuracy: 0.8359 - loss: 0.4876 - val_accuracy: 0.8350 - val_loss: 0.5114\n",
      "Epoch 25/25\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m166s\u001b[0m 2s/step - accuracy: 0.8184 - loss: 0.5145 - val_accuracy: 0.8750 - val_loss: 0.4038\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x264599b04d0>"
      ]
     },
     "execution_count": 619,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(dataset_train, epochs =25, validation_data = dataset_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "id": "c5d5490c-4e80-4baf-83aa-3e6c3b8d7a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 705,
   "id": "9b40a5b1-f7cc-4cf9-a806-bcd24835c8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.set_weights(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 669,
   "id": "67b18377-b304-41d5-ba13-ecf8dfae7525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 1s/step - accuracy: 0.8475 - loss: 0.5101\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 707,
   "id": "46375f1f-49b9-486e-9b30-b0dfa5dfe2b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 1s/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 708,
   "id": "4511e59e-1082-4fe2-b0ee-3295778d9fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities = tf.nn.softmax(y_pred).numpy()\n",
    "predicted_classes = np.argmax(probabilities, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 709,
   "id": "8f9b53eb-2bb3-448d-a271-671388014f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_classes = np.array(predicted_classes, dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 710,
   "id": "aeea2ebf-2d85-4f52-9033-4abc9b5a9202",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = np.array(y_true, dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 711,
   "id": "ddad8874-5dc9-41be-b52d-2e8e631fa751",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = []\n",
    "for image, label in dataset_test:\n",
    "    for i in range(len(label)):\n",
    "        y_true.append(label[i].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 712,
   "id": "dbf1a20e-bf19-4ba1-bd98-948a578749f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_classes = predicted_classes.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 713,
   "id": "ec3a266b-146c-4848-9ad8-23138481e726",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 714,
   "id": "606c48cd-cc2a-485c-829a-12bc2cc45a50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8476366547853702"
      ]
     },
     "execution_count": 714,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_true, predicted_classes, average = 'weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0078edb5-9f72-4af3-a956-cca29db2986a",
   "metadata": {},
   "source": [
    "## Model Performance and Improvement\n",
    "\n",
    "### Performance Results\n",
    "\n",
    "After training the model, we obtained the following results:\n",
    "- **Training Accuracy**: 82%\n",
    "  - Indicates how well the model performs on the data it was trained on.\n",
    "  \n",
    "- **Validation Accuracy**: 87.5%\n",
    "  - Shows how well the model generalizes to new, unseen data during training. This suggests that the model is effectively learning and not overfitting.\n",
    "\n",
    "- **Test Accuracy**: 85%\n",
    "  - Reflects the model's performance on completely unseen data. This accuracy demonstrates the model’s ability to generalize well to new images.\n",
    "\n",
    "- **F1 Score**: 0.85\n",
    "  - An F1 score of 0.85 indicates strong performance across all classes, particularly important given the class imbalance in the dataset. The F1 score balances precision and recall, showing that the model performs well even on less frequent classes.\n",
    "\n",
    "### Potential Improvement\n",
    "\n",
    "To further enhance model performance, especially for categories with fewer samples, consider implementing image augmentation:\n",
    "- **Image Augmentation**: Apply techniques such as rotation, flipping, scaling, and color adjustments to artificially increase the number of samples in underrepresented categories. \n",
    "  - This can help the model learn more robust features and improve generalization for categories with fewer samples.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "01c9736c-f233-43fb-8f55-592abcc9ac66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_image(image):\n",
    "    image = tf.image.random_brightness(image, max_delta=0.3)\n",
    "    image = tf.image.random_hue(image, max_delta=0.2)\n",
    "    image = tf.image.random_saturation(image, lower=0.7, upper=1.3)\n",
    "    image = tf.image.random_contrast(image, lower=0.7, upper=1.3)\n",
    "    return image\n",
    "def augment_and_collect(dataset, label_to_augment, num_augmentations):\n",
    "    augmented_images = []\n",
    "    augmented_labels = []\n",
    "    \n",
    "    for image, label in dataset:\n",
    "        if label == label_to_augment:\n",
    "            for _ in range(num_augmentations):\n",
    "                aug_image = augment_image(image)\n",
    "                augmented_images.append(aug_image)\n",
    "                augmented_labels.append(label)\n",
    "        else:\n",
    "            augmented_images.append(image)\n",
    "            augmented_labels.append(label)\n",
    "\n",
    "    return tf.data.Dataset.from_tensor_slices((augmented_images, augmented_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6b9cfd-a937-47df-ab99-21eb8fb6e0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train_augmented = dataset_train.unbatch()\n",
    "dataset_train_augmented = augment_and_collect(dataset_train_augmented, 5, 10)\n",
    "dataset_train_augmented = dataset_train_augmented.batch(32).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 725,
   "id": "32e467e9-3c6c-4a56-b2cd-ceb5bccb9356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106\n"
     ]
    }
   ],
   "source": [
    "c = 0\n",
    "for image, label in dataset_train_augmented:\n",
    "    c = c+1\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 679,
   "id": "0c0c2787-8f2a-4971-a2b0-790aa2bb585c",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.Adam(learning_rate=0.003)\n",
    "model.compile(loss = SparseCategoricalCrossentropy(from_logits = True), optimizer=opt, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 681,
   "id": "b0cd566f-e066-4d43-be8c-619c0b6a0437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m106/106\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 2s/step - accuracy: 0.7546 - loss: 0.7167 - val_accuracy: 0.8730 - val_loss: 0.4680\n",
      "Epoch 2/15\n",
      "\u001b[1m106/106\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m198s\u001b[0m 2s/step - accuracy: 0.7675 - loss: 0.6293 - val_accuracy: 0.8620 - val_loss: 0.4729\n",
      "Epoch 3/15\n",
      "\u001b[1m106/106\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m205s\u001b[0m 2s/step - accuracy: 0.7707 - loss: 0.6161 - val_accuracy: 0.8670 - val_loss: 0.4931\n",
      "Epoch 4/15\n",
      "\u001b[1m106/106\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m196s\u001b[0m 2s/step - accuracy: 0.7646 - loss: 0.6155 - val_accuracy: 0.8570 - val_loss: 0.5242\n",
      "Epoch 5/15\n",
      "\u001b[1m106/106\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m227s\u001b[0m 2s/step - accuracy: 0.7745 - loss: 0.6300 - val_accuracy: 0.8700 - val_loss: 0.4794\n",
      "Epoch 6/15\n",
      "\u001b[1m106/106\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m255s\u001b[0m 2s/step - accuracy: 0.7722 - loss: 0.6057 - val_accuracy: 0.8640 - val_loss: 0.5046\n",
      "Epoch 7/15\n",
      "\u001b[1m106/106\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m231s\u001b[0m 2s/step - accuracy: 0.7679 - loss: 0.6235 - val_accuracy: 0.8380 - val_loss: 0.5802\n",
      "Epoch 8/15\n",
      "\u001b[1m106/106\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m230s\u001b[0m 2s/step - accuracy: 0.7600 - loss: 0.6502 - val_accuracy: 0.8570 - val_loss: 0.5208\n",
      "Epoch 9/15\n",
      "\u001b[1m106/106\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m228s\u001b[0m 2s/step - accuracy: 0.7724 - loss: 0.6079 - val_accuracy: 0.8570 - val_loss: 0.5226\n",
      "Epoch 10/15\n",
      "\u001b[1m106/106\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m185s\u001b[0m 2s/step - accuracy: 0.7737 - loss: 0.6125 - val_accuracy: 0.8630 - val_loss: 0.5131\n",
      "Epoch 11/15\n",
      "\u001b[1m106/106\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m204s\u001b[0m 2s/step - accuracy: 0.7732 - loss: 0.6061 - val_accuracy: 0.8370 - val_loss: 0.5421\n",
      "Epoch 12/15\n",
      "\u001b[1m106/106\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m197s\u001b[0m 2s/step - accuracy: 0.7778 - loss: 0.5950 - val_accuracy: 0.8510 - val_loss: 0.5172\n",
      "Epoch 13/15\n",
      "\u001b[1m106/106\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m189s\u001b[0m 2s/step - accuracy: 0.7768 - loss: 0.5987 - val_accuracy: 0.8570 - val_loss: 0.4889\n",
      "Epoch 14/15\n",
      "\u001b[1m106/106\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m185s\u001b[0m 2s/step - accuracy: 0.7722 - loss: 0.5873 - val_accuracy: 0.8450 - val_loss: 0.5418\n",
      "Epoch 15/15\n",
      "\u001b[1m106/106\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m191s\u001b[0m 2s/step - accuracy: 0.7628 - loss: 0.6263 - val_accuracy: 0.8520 - val_loss: 0.5283\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x26473bab710>"
      ]
     },
     "execution_count": 681,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(dataset_train_augmented, epochs =15, validation_data = dataset_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 683,
   "id": "052ab696-a91a-4560-b910-8f585a4b2447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 1s/step - accuracy: 0.8023 - loss: 0.6330\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5693063735961914, 0.8199999928474426]"
      ]
     },
     "execution_count": 683,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 685,
   "id": "99277af9-2859-4962-98f3-21462ff27cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights1  = model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 687,
   "id": "83d4972f-c12b-4478-8c4e-a68c35354270",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.set_weights(weights1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd9a9f6-3c34-4420-841c-dd8c7bec9f93",
   "metadata": {},
   "source": [
    "## Model Re-Evaluation After Image Augmentation\r\n",
    "\r\n",
    "### Model Training with Augmented Images\r\n",
    "\r\n",
    "After applying image augmentation techniques to increase the number of samples for underrepresented categories, we retrained the model starting from the previously trained version. This approach was chosen to leverage the pre-learned features and improve convergence.\r\n",
    "\r\n",
    "### Performance Results\r\n",
    "\r\n",
    "Upon evaluating the retrained model with augmented images, the following results were observed:\r\n",
    "- **Validation Accuracy**: Decreased to 85%\r\n",
    "  - The validation accuracy, which was previously 87.5%, dropped to 85%. This suggests that the model did not generalize as well to the validation data after augmentation.\r\n",
    "\r\n",
    "- **Test Accuracy**: Decreased to 82%\r\n",
    "  - The test set accuracy, which was previously 85%, fell to 82%. This decline indicates that the model's performance on unseen data has also decreased.\r\n",
    "\r\n",
    "### Analysis\r\n",
    "\r\n",
    "- **Original Model Performance**: The original model, which was trained without image augmentation, achieved higher accuracy on both the validation and test sets.\r\n",
    "- **Impact of Augmentation**: The decrease in accuracy suggests that while augmentation aims to address class imbalance, it may have introduced noise or altered the data distribution in a way that negatively impacted the model’s performance.\r\n",
    "\r\n",
    "### Conclusion\r\n",
    "\r\n",
    "Despite the intention to improve the model's performance through augmentation, the results indicate that the original model without augmentation performed better on both validation and test datasets. Further analysis might be needed to refine augmentation techniques or explore other methods for handling class imbalance.\r\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
